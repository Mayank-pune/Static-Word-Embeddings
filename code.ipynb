{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import random\n",
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "\n",
    "#############################################\n",
    "# Data Preparation\n",
    "#############################################\n",
    "\n",
    "def get_corpus():\n",
    "\n",
    "    nltk.download('brown')\n",
    "    corpus = [list(map(str.lower, sent)) for sent in brown.sents()]\n",
    "    return corpus\n",
    "\n",
    "def build_vocab(corpus, min_count=5):\n",
    "    word_counts = Counter()\n",
    "    for sent in corpus:\n",
    "        word_counts.update(sent)\n",
    "\n",
    "    vocab = [w for w, cnt in word_counts.items() if cnt >= min_count]\n",
    "    \n",
    "    vocab = [\"<pad>\", \"<unk>\"] + vocab  # Ensure <pad> and <unk> have lowest indices\n",
    "    \n",
    "    word2id = {w: i for i, w in enumerate(vocab)}\n",
    "    id2word = {i: w for w, i in word2id.items()}\n",
    "\n",
    "    return vocab, word2id, id2word\n",
    "\n",
    "def find_top_k_similar_words(word, k, embeddings, word2id, id2word):\n",
    "    if word not in word2id:\n",
    "        print(f\"Word '{word}' not in vocabulary.\")\n",
    "        \n",
    "        return []\n",
    "\n",
    "    word_idx = word2id[word]\n",
    "    word_vector = embeddings[word_idx].unsqueeze(0)  # Shape (1, embedding_dim)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarities = torch.nn.functional.cosine_similarity(word_vector, embeddings)\n",
    "    \n",
    "    # Get top k indices (excluding the word itself)\n",
    "    top_k_indices = similarities.argsort(descending=True)[1:k+1]\n",
    "    \n",
    "    # Map indices back to words\n",
    "    top_k_words = [id2word[idx.item()] for idx in top_k_indices]\n",
    "    \n",
    "    return top_k_words\n",
    "\n",
    "#############################################\n",
    "# 1. SVD Implementation\n",
    "#############################################\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "\n",
    "\n",
    "#############################################\n",
    "# 4. Word Similarity Evaluation (WordSim-353)\n",
    "#############################################\n",
    "\n",
    "def evaluate_wordsim(embeddings, word2id, wordsim_file='wordsim353.csv'):\n",
    "    \"\"\"\n",
    "    Assumes wordsim_file is a CSV with columns: 'Word 1', 'Word 2', 'Human (mean)'.\n",
    "    Only evaluates pairs where both words are in word2id.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(wordsim_file)\n",
    "    computed_sims = []\n",
    "    human_scores = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        w1 = row['Word 1'].lower()  # ensure lowercase for matching\n",
    "        w2 = row['Word 2'].lower()\n",
    "        score = row['Human (Mean)']\n",
    "        if w1 in word2id and w2 in word2id:\n",
    "            vec1 = embeddings[word2id[w1]].unsqueeze(0)\n",
    "            vec2 = embeddings[word2id[w2]].unsqueeze(0)\n",
    "            sim = F.cosine_similarity(vec1, vec2).item()\n",
    "            computed_sims.append(sim)\n",
    "            human_scores.append(score)\n",
    "    if computed_sims:\n",
    "        correlation, _ = spearmanr(computed_sims, human_scores)\n",
    "        print(f\"Spearman Correlation: {correlation:.4f}\")\n",
    "    else:\n",
    "        print(\"No word pairs found in embeddings for evaluation.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_svd(corpus, vocab, window_size=2, embed_size=100, device=torch.device(\"cpu\")):\n",
    "    vocab_size = len(vocab)\n",
    "    word2id = {word: i for i, word in enumerate(vocab)}\n",
    "    UNK_ID = word2id[\"<unk>\"]\n",
    "    PAD_ID = word2id[\"<pad>\"]\n",
    "\n",
    "\n",
    "    # Build sparse co-occurrence matrix using coordinate format (COO)\n",
    "    data = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "\n",
    "    for sentence in corpus:\n",
    "        sentence = [word if word in word2id else \"<unk>\" for word in sentence]\n",
    "        indices = [word2id[word] for word in sentence]\n",
    "\n",
    "        # Pad for window size at sentence boundaries\n",
    "        padded_indices = [PAD_ID] * window_size + indices + [PAD_ID] * window_size\n",
    "\n",
    "        for i in range(window_size, len(padded_indices) - window_size):\n",
    "            center = padded_indices[i]\n",
    "            for j in range(-window_size, window_size + 1):\n",
    "                if j != 0:  # Skip center word itself\n",
    "                    rows.append(center)\n",
    "                    cols.append(padded_indices[i + j])\n",
    "                    data.append(1)\n",
    "    from scipy.sparse import coo_matrix\n",
    "    cooc_matrix = coo_matrix((data, (rows, cols)), shape=(vocab_size, vocab_size))\n",
    "    \n",
    "    # Compute truncated SVD on the sparse matrix\n",
    "    U, s, Vt = svds(cooc_matrix, k=embed_size)\n",
    "    # svds does not guarantee sorted order; sort singular values/vectors in descending order\n",
    "    idx = np.argsort(s)[::-1]\n",
    "    s = s[idx]\n",
    "    U = U[:, idx]\n",
    "    \n",
    "    # Form the word embeddings (scale U by sqrt(singular values))\n",
    "    embeddings = np.dot(U, np.diag(np.sqrt(s)))\n",
    "    embeddings_tensor = torch.tensor(embeddings, dtype=torch.float).to(device)\n",
    "    torch.save(embeddings_tensor, 'svd_embeddings.pt')\n",
    "    print(\"SVD embeddings saved as svd_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/cool_mayank/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Select device: GPU if available, else CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "corpus = get_corpus()\n",
    "vocab, word2id, id2word = build_vocab(corpus, min_count=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD embeddings saved as svd_embeddings.pt\n",
      "\n",
      "Evaluating SVD Embeddings:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51071/1766565079.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  svd_embeddings = torch.load('svd_embeddings.pt', map_location=device)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'wordsim353.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m svd_word2id \u001b[38;5;241m=\u001b[39m {w: i \u001b[38;5;28;01mfor\u001b[39;00m i, w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vocab)}\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating SVD Embeddings:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m evaluate_wordsim(svd_embeddings, svd_word2id, wordsim_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordsim353.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 77\u001b[0m, in \u001b[0;36mevaluate_wordsim\u001b[0;34m(embeddings, word2id, wordsim_file)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_wordsim\u001b[39m(embeddings, word2id, wordsim_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordsim353.csv\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     73\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    Assumes wordsim_file is a CSV with columns: 'Word 1', 'Word 2', 'Human (mean)'.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    Only evaluates pairs where both words are in word2id.\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(wordsim_file)\n\u001b[1;32m     78\u001b[0m     computed_sims \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     79\u001b[0m     human_scores \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'wordsim353.csv'"
     ]
    }
   ],
   "source": [
    "# 1. Train SVD-based embeddings\n",
    "train_svd(corpus, vocab, window_size=2, embed_size=100, device=device)\n",
    "\n",
    "\n",
    "svd_embeddings = torch.load('svd_embeddings.pt', map_location=device)\n",
    "svd_word2id = {w: i for i, w in enumerate(vocab)}\n",
    "print(\"\\nEvaluating SVD Embeddings:\")\n",
    "evaluate_wordsim(svd_embeddings, svd_word2id, wordsim_file='wordsim353.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 4536/4536 [00:55<00:00, 81.48it/s, loss=2.38]\n",
      "Epoch 2: 100%|██████████| 4536/4536 [00:55<00:00, 81.53it/s, loss=1.99]\n",
      "Epoch 3: 100%|██████████| 4536/4536 [00:55<00:00, 81.52it/s, loss=1.86]\n",
      "Epoch 4: 100%|██████████| 4536/4536 [00:55<00:00, 81.30it/s, loss=1.77]\n",
      "Epoch 5: 100%|██████████| 4536/4536 [00:56<00:00, 80.40it/s, loss=1.69]\n",
      "Epoch 6: 100%|██████████| 4536/4536 [00:56<00:00, 80.71it/s, loss=1.64]\n",
      "Epoch 7: 100%|██████████| 4536/4536 [00:56<00:00, 80.66it/s, loss=1.58]\n",
      "Epoch 8: 100%|██████████| 4536/4536 [00:56<00:00, 80.48it/s, loss=1.54]\n",
      "Epoch 9: 100%|██████████| 4536/4536 [00:57<00:00, 78.45it/s, loss=1.5] \n",
      "Epoch 10: 100%|██████████| 4536/4536 [00:56<00:00, 79.79it/s, loss=1.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW embeddings saved as cbow_embeddings.pt\n",
      "\n",
      "Evaluating CBOW Embeddings:\n",
      "Spearman Correlation: 0.1653\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CBOWDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, corpus, word2id, window_size=2, num_negative=5, neg_sample_pool_size=100000):\n",
    "        self.data = []\n",
    "        self.word2id = word2id\n",
    "        self.window_size = window_size\n",
    "        self.num_negative = num_negative\n",
    "        self.pad_id = word2id[\"<pad>\"]\n",
    "        self.unk_id = word2id[\"<unk>\"]\n",
    "        self.neg_sample_pool_size = neg_sample_pool_size\n",
    "\n",
    "        # Compute word frequencies for negative sampling\n",
    "        self.word_freq = Counter(word for sent in corpus for word in sent if word in word2id)\n",
    "        valid_words = [w for w in self.word_freq.keys() if w in word2id]\n",
    "\n",
    "        # Build negative sampling probabilities\n",
    "        total = sum(self.word_freq[w]**0.75 for w in valid_words)\n",
    "        self.neg_probs = np.array([(self.word_freq[w]**0.75) / total for w in valid_words])\n",
    "        self.word_list = [word2id[w] for w in valid_words]  # Convert words to IDs\n",
    "\n",
    "        # Precompute negative samples\n",
    "        self.negative_samples = np.random.choice(self.word_list, size=self.neg_sample_pool_size, p=self.neg_probs)\n",
    "        self.neg_index = 0  # Pointer for sampling negatives\n",
    "\n",
    "        # Create (context, target) pairs\n",
    "        for sent in corpus:\n",
    "            sent = [word if word in word2id else \"<unk>\" for word in sent]\n",
    "            indices = [word2id[word] for word in sent]\n",
    "\n",
    "            # Pad at sentence boundaries\n",
    "            padded_indices = [self.pad_id] * window_size + indices + [self.pad_id] * window_size\n",
    "\n",
    "            for i in range(window_size, len(padded_indices) - window_size):\n",
    "                target = padded_indices[i]\n",
    "                context = [padded_indices[j] for j in range(i - window_size, i + window_size + 1) if j != i]\n",
    "                self.data.append((context, target))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context, target = self.data[idx]\n",
    "\n",
    "        # Fetch negative samples from the precomputed pool\n",
    "        neg_end = self.neg_index + self.num_negative\n",
    "        if neg_end > len(self.negative_samples):  # If exceeding buffer, reshuffle\n",
    "            np.random.shuffle(self.negative_samples)\n",
    "            self.neg_index = 0\n",
    "            neg_end = self.num_negative\n",
    "        negatives = self.negative_samples[self.neg_index:neg_end]\n",
    "        self.neg_index += self.num_negative  # Move index forward\n",
    "\n",
    "        return (torch.tensor(context, dtype=torch.long), \n",
    "                torch.tensor(target, dtype=torch.long), \n",
    "                torch.tensor(negatives, dtype=torch.long))\n",
    "\n",
    "\n",
    "\n",
    "def cbow_collate(batch):\n",
    "    contexts, targets, negatives = zip(*batch)\n",
    "    contexts_padded = torch.nn.utils.rnn.pad_sequence(contexts, batch_first=True, padding_value=0)\n",
    "    lengths = torch.tensor([len(c) for c in contexts])\n",
    "    targets = torch.stack(targets)\n",
    "    negatives = torch.stack(negatives)\n",
    "    return contexts_padded, lengths, targets, negatives\n",
    "\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, pad_idx):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        self.in_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.init_embeddings()\n",
    "    \n",
    "    def init_embeddings(self):\n",
    "        initrange = 0.5 / self.embedding_dim\n",
    "        self.in_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_embeddings.weight.data.uniform_(0, 0)\n",
    "    \n",
    "    def forward(self, contexts, lengths, targets, negatives):\n",
    "        embeds = self.in_embeddings(contexts)  # (batch, max_context_length, embed_dim)\n",
    "        mask = (contexts != 0).unsqueeze(2).float()\n",
    "        summed = torch.sum(embeds * mask, dim=1)\n",
    "        lengths = lengths.unsqueeze(1).float()\n",
    "        context_embeds = summed / lengths\n",
    "        \n",
    "        target_embeds = self.out_embeddings(targets)           # (batch, embed_dim)\n",
    "        neg_embeds = self.out_embeddings(negatives)            # (batch, num_negative, embed_dim)\n",
    "        \n",
    "        pos_score = torch.sum(context_embeds * target_embeds, dim=1)\n",
    "        pos_loss = -torch.log(torch.sigmoid(pos_score) + 1e-10)\n",
    "        \n",
    "        neg_score = torch.bmm(neg_embeds, context_embeds.unsqueeze(2)).squeeze(2)\n",
    "        neg_loss = -torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-10), dim=1)\n",
    "        \n",
    "        loss = pos_loss + neg_loss\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def train_cbow(corpus, word2id, embedding_dim=100, window_size=2, num_negative=5, epochs=5, batch_size=32):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset = CBOWDataset(corpus, word2id, window_size, num_negative)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=cbow_collate)\n",
    "\n",
    "    pad_id = word2id[\"<pad>\"]\n",
    "    model = CBOWModel(len(word2id), embedding_dim, pad_idx=pad_id).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "        for contexts, lengths, targets, negatives in progress_bar:\n",
    "            contexts, lengths, targets, negatives = contexts.to(device), lengths.to(device), targets.to(device), negatives.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(contexts, lengths, targets, negatives)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=total_loss / (progress_bar.n + 1))\n",
    "\n",
    "    \n",
    "    torch.save(model.in_embeddings.state_dict(), 'cbow_embeddings_non_dyn.pt')\n",
    "    print(\"CBOW embeddings saved as cbow_embeddings.pt\")\n",
    "\n",
    "\n",
    "\n",
    "# # 2. Train CBOW model embeddings\n",
    "train_cbow(corpus, word2id, embedding_dim=100, window_size=2, num_negative=5, epochs=10, batch_size=256)\n",
    "cbow_state = torch.load('cbow_embeddings_non_dyn.pt', map_location=device)\n",
    "\n",
    "# If cbow_state is an OrderedDict, use it directly\n",
    "if isinstance(cbow_state, dict):\n",
    "    cbow_layer = nn.Embedding.from_pretrained(cbow_state['weight']).to(device)\n",
    "else:\n",
    "    cbow_layer = nn.Embedding(len(word2id) + 1, 100).to(device)\n",
    "    cbow_layer.load_state_dict({'weight': cbow_state})\n",
    "\n",
    "print(\"\\nEvaluating CBOW Embeddings:\")\n",
    "evaluate_wordsim(cbow_layer.weight.data, word2id, wordsim_file='wordsim353.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 16803, number of samples: 4301382, corpus length: 57340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-Gram Epoch 1, Loss: 2.3159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-Gram Epoch 2, Loss: 2.1599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-Gram Epoch 3, Loss: 2.1026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-Gram Epoch 4, Loss: 2.0622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-Gram Epoch 5, Loss: 2.0294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-Gram Epoch 6, Loss: 2.0028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-Gram Epoch 7, Loss: 1.9823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-Gram Epoch 8, Loss: 1.9649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-Gram Epoch 9, Loss: 1.9516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-Gram Epoch 10, Loss: 1.9412\n",
      "Skip-Gram embeddings saved as skipgram_embeddings.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class SkipGramDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, corpus, word2id, window_size=2, num_negative=5, neg_sample_pool_size=100000):\n",
    "        self.data = []\n",
    "        self.word2id = word2id\n",
    "        self.window_size = window_size\n",
    "        self.num_negative = num_negative\n",
    "        self.neg_sample_pool_size = neg_sample_pool_size\n",
    "\n",
    "        UNK_ID = word2id[\"<unk>\"]\n",
    "        PAD_ID = word2id[\"<pad>\"]\n",
    "\n",
    "        # Compute word frequencies only for words in vocabulary\n",
    "        self.word_freq = Counter()\n",
    "        for sent in corpus:\n",
    "            for word in sent:\n",
    "                word_id = word2id.get(word, UNK_ID)  # Replace OOV with <unk>\n",
    "                if word_id != PAD_ID:  # Ignore <pad>\n",
    "                    self.word_freq[word] += 1\n",
    "\n",
    "        # Only keep words that exist in word2id\n",
    "        valid_words = [w for w in self.word_freq.keys() if w in word2id]\n",
    "\n",
    "        # Build negative sampling probabilities\n",
    "        total = sum(self.word_freq[w]**0.75 for w in valid_words)\n",
    "        self.neg_probs = np.array([(self.word_freq[w]**0.75) / total for w in valid_words])\n",
    "        self.word_list = [word2id[w] for w in valid_words]  # Convert to IDs for fast lookup\n",
    "\n",
    "        # Precompute negative samples\n",
    "        self.negative_samples = np.random.choice(self.word_list, size=self.neg_sample_pool_size, p=self.neg_probs)\n",
    "        self.neg_index = 0  # Pointer to track position in the pool\n",
    "\n",
    "        # Create (center, context) pairs\n",
    "        for sent in corpus:\n",
    "            sent = [word if word in word2id else \"<unk>\" for word in sent]  # Replace OOVs\n",
    "            indices = [word2id[word] for word in sent if word2id[word] != PAD_ID]  # Remove <pad>\n",
    "\n",
    "            for i, center in enumerate(indices):\n",
    "                for j in range(max(0, i - window_size), min(len(indices), i + window_size + 1)):\n",
    "                    if j != i:\n",
    "                        context = indices[j]\n",
    "                        self.data.append((center, context))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        center, context = self.data[idx]\n",
    "\n",
    "        # Fetch negative samples from the precomputed pool\n",
    "        neg_end = self.neg_index + self.num_negative\n",
    "        if neg_end > len(self.negative_samples):  # If exceeding buffer, reshuffle\n",
    "            np.random.shuffle(self.negative_samples)\n",
    "            self.neg_index = 0\n",
    "            neg_end = self.num_negative\n",
    "        negatives = self.negative_samples[self.neg_index:neg_end]\n",
    "        self.neg_index += self.num_negative  # Move index forward\n",
    "\n",
    "        return (torch.tensor(center, dtype=torch.long), \n",
    "                torch.tensor(context, dtype=torch.long), \n",
    "                torch.tensor(negatives, dtype=torch.long))\n",
    "\n",
    "\n",
    "class SkipGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.in_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.init_embeddings()\n",
    "    \n",
    "    def init_embeddings(self):\n",
    "        initrange = 0.5 / self.embedding_dim\n",
    "        self.in_embeddings.weight.data.uniform_(-initrange, initrange)\n",
    "        self.out_embeddings.weight.data.uniform_(0, 0)\n",
    "    \n",
    "    def forward(self, centers, targets, negatives):\n",
    "        center_embeds = self.in_embeddings(centers)    # (batch, embed_dim)\n",
    "        target_embeds = self.out_embeddings(targets)   # (batch, embed_dim)\n",
    "        neg_embeds = self.out_embeddings(negatives)    # (batch, num_negative, embed_dim)\n",
    "        \n",
    "        pos_score = torch.sum(center_embeds * target_embeds, dim=1)\n",
    "        pos_loss = -torch.log(torch.sigmoid(pos_score) + 1e-10)\n",
    "        \n",
    "        neg_score = torch.bmm(neg_embeds, center_embeds.unsqueeze(2)).squeeze(2)\n",
    "        neg_loss = -torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-10), dim=1)\n",
    "        \n",
    "        loss = pos_loss + neg_loss\n",
    "        return loss.mean()\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_skipgram(corpus, word2id, embedding_dim=100, window_size=2, num_negative=2, epochs=5, batch_size=32):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dataset = SkipGramDataset(corpus, word2id, window_size, num_negative)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    print(f\"Number of batches: {len(dataloader)}, number of samples: {len(dataset)}, corpus length: {len(corpus)}\")\n",
    "    model = SkipGramModel(len(word2id), embedding_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "        \n",
    "        for centers, targets, negatives in progress_bar:\n",
    "            centers, targets, negatives = centers.to(device), targets.to(device), negatives.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = model(centers, targets, negatives)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=total_loss / (progress_bar.n + 1))\n",
    "        \n",
    "        print(f\"Skip-Gram Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    torch.save(model.in_embeddings.state_dict(), 'skipgram_embeddings.pt')\n",
    "    print(\"Skip-Gram embeddings saved as skipgram_embeddings.pt\")\n",
    "    \n",
    "# 3. Train Skip-Gram model embeddings\n",
    "train_skipgram(corpus, word2id, embedding_dim=100, window_size=2, num_negative=5, epochs=10, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Skip-Gram Embeddings:\n",
      "Spearman Correlation: 0.2215\n"
     ]
    }
   ],
   "source": [
    "skipgram_state = torch.load('skipgram_embeddings.pt', map_location=device)\n",
    "skipgram_layer = nn.Embedding.from_pretrained(skipgram_state['weight']).to(device)\n",
    "\n",
    "print(\"\\nEvaluating Skip-Gram Embeddings:\")\n",
    "evaluate_wordsim(nn.Embedding.from_pretrained(skipgram_state['weight']).weight.data, word2id, wordsim_file='wordsim353.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis of the Three Models\n",
    "\n",
    "### 1. SkipGram\n",
    "\n",
    "| Embedding Dimension | Spearman Correlation |\n",
    "|---------------------|---------------------|\n",
    "| 64                 | 0.1900              |\n",
    "| 128                | 0.3417              |\n",
    "| 256                | 0.3512              |\n",
    "| 512                | 0.3128              |\n",
    "\n",
    "**Observations:**  \n",
    "- SkipGram shows a strong improvement from **64 → 128** dimensions and peaks at **256** dimensions.  \n",
    "- The slight drop at **512** dims suggests overfitting, optimization issues, or diminishing returns.  \n",
    "- Overall, SkipGram consistently captures semantic similarity better than the other two methods at most dimensional settings.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. CBOW\n",
    "\n",
    "| Embedding Dimension | Spearman Correlation |\n",
    "|---------------------|---------------------|\n",
    "| 64                 | 0.1750              |\n",
    "| 128                | 0.1872              |\n",
    "| 256                | 0.2187              |\n",
    "| 512                | 0.2623              |\n",
    "\n",
    "**Observations:**  \n",
    "- CBOW improves steadily with increasing dimensions.  \n",
    "- While it starts lower than SkipGram, the gap narrows at higher dimensions (**512** dims).  \n",
    "- CBOW benefits from additional dimensions by gradually capturing more semantic relationships.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. SVD (Count-based Method)\n",
    "\n",
    "| Embedding Dimension | Spearman Correlation |\n",
    "|---------------------|---------------------|\n",
    "| 64                 | 0.0349              |\n",
    "| 128                | 0.0783              |\n",
    "| 256                | 0.1048              |\n",
    "| 512                | 0.1458              |\n",
    "\n",
    "**Observations:**  \n",
    "- SVD consistently yields the lowest Spearman correlations compared to the predictive models.  \n",
    "- However, it does improve steadily with larger embedding dimensions.  \n",
    "- Despite improvements, it remains less effective at capturing nuanced contextual relationships compared to neural approaches.\n",
    "\n",
    "---\n",
    "\n",
    "## Overall Trends\n",
    "\n",
    "### **Impact of Dimensionality**\n",
    "For all models, increasing the embedding dimension tends to improve performance. However, the magnitude of improvement varies:\n",
    "- **SkipGram** benefits significantly from **64 → 128** dimensions, peaking at **256**.\n",
    "- **CBOW** improves steadily and becomes more competitive at **512** dimensions.\n",
    "- **SVD** shows improvement but still lags behind the predictive models at all dimensions.\n",
    "\n",
    "### **Model Comparison**\n",
    "| Model  | Best Performance (Spearman) | Best Dimension |\n",
    "|--------|-----------------------------|---------------|\n",
    "| SkipGram | 0.3512 | 256 |\n",
    "| CBOW | 0.2623 | 512 |\n",
    "| SVD | 0.1458 | 512 |\n",
    "\n",
    "- **SkipGram** leads in capturing semantic similarity, especially at moderate dimensions (**128–256**).\n",
    "- **CBOW** closes the gap at higher dimensions, suggesting it benefits more from increased capacity.\n",
    "- **SVD**, while improving, remains behind the neural models in effectiveness.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "The results indicate that predictive models (**SkipGram** and **CBOW**) outperform the **SVD** approach in capturing word similarity, with **SkipGram** showing the best performance at moderate embedding sizes (**128–256**). **CBOW**, while initially trailing, improves with higher dimensions. **SVD**, despite its improvements, remains less effective in this metric.\n",
    "\n",
    "Link to all csv_results files and pt files: [text](https://drive.google.com/drive/folders/1bBFfzxD0TTrtN6nfVtOi4GVgVrE2I2vC?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
